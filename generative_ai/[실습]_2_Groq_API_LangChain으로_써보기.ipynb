{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOmZwsI1kTrd"
   },
   "source": [
    "# Groq API 써보기\n",
    "\n",
    "Groq의 빠른 추론 엔진을 LangChain에서 체험하는 코드를 소개합니다.    \n",
    "langchain_groq를 통해 쉽게 연결할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "daSSN5ZSetZl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.18-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain_groq\n",
      "  Downloading langchain_groq-0.2.4-py3-none-any.whl (14 kB)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pymupdf\n",
      "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pypdf\n",
      "  Downloading pypdf-5.3.0-py3-none-any.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 KB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.0/186.0 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
      "Collecting langchain-core<1.0.0,>=0.3.34\n",
      "  Downloading langchain_core-0.3.35-py3-none-any.whl (413 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.2/413.2 KB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.6\n",
      "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17\n",
      "  Downloading langsmith-0.3.8-py3-none-any.whl (332 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 KB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy<2,>=1.26.4\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting groq<1,>=0.4.1\n",
      "  Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 KB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 KB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 KB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 KB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (24.2)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 KB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zstandard<0.24.0,>=0.23.0\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: zstandard, soupsieve, pypdf, pymupdf, propcache, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, httpx-sse, frozenlist, async-timeout, aiohappyeyeballs, yarl, typing-inspect, requests-toolbelt, jsonpatch, beautifulsoup4, aiosignal, pydantic-settings, langsmith, groq, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain_groq, langchain, langchain_community\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 async-timeout-4.0.3 beautifulsoup4-4.13.3 dataclasses-json-0.6.7 frozenlist-1.5.0 groq-0.18.0 httpx-sse-0.4.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.18 langchain-core-0.3.35 langchain-text-splitters-0.3.6 langchain_community-0.3.17 langchain_groq-0.2.4 langsmith-0.3.8 marshmallow-3.26.1 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.15 propcache-0.2.1 pydantic-settings-2.7.1 pymupdf-1.25.3 pypdf-5.3.0 requests-toolbelt-1.0.0 soupsieve-2.6 typing-inspect-0.9.0 yarl-1.18.3 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_groq langchain_community pymupdf pypdf beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo6qR14UoZEu"
   },
   "source": [
    "현재 사용 가능한 주요 모델과 API 제한은 아래와 같습니다.\n",
    "\n",
    "\n",
    "| ID                                     | Requests per Minute | Requests per Day | Tokens per Minute | Tokens per Day |\n",
    "|----------------------------------------|---------------------|------------------|-------------------|----------------|\n",
    "| gemma-7b-it                            | 30                  | 14,400           | 15,000            | 500,000        |\n",
    "| gemma2-9b-it                           | 30                  | 14,400           | 15,000            | 500,000        |\n",
    "| llama-3.1-70b-versatile                | 30                  | 14,400           | 18,000            | 500,000        |\n",
    "| llama-3.1-8b-instant                   | 30                  | 14,400           | 20,000            | 500,000        |\n",
    "| llama-3.2-11b-text-preview             | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-11b-vision-preview           | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-1b-preview                   | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-3b-preview                   | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-90b-text-preview             | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-90b-vision-preview           | 15                  | 3,500            | 7,000             | 250,000        |\n",
    "| llava-v1.5-7b-4096-preview             | 30                  | 14,400           | 30,000            | (No limit)     |\n",
    "| mixtral-8x7b-32768                     | 30                  | 14,400           | 5,000             | 500,000        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZozwH3G_kPDc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq # Groq-LangChain 연결\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcNUf5TCpZTe"
   },
   "source": [
    "간단한 체인을 만들고 실행합니다.\n",
    "\n",
    "https://console.groq.com/keys 에서 키를 생성해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NVj9S2kZeThO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq 엔진의 LLM(Large Language Model) 추론 속도가 빠른 이유는 Groq Language Processing Unit(GPU)가 설계된 목적이 AI 추론과 언어 처리에 최적화된 것 때문입니다. GPU는 원래 그래픽 처리를 위해 설계되었지만, Groq LPU는 AI 추론과 언어 처리를 위해 설계되었습니다.\n",
      "\n",
      "Groq LPU는 다음과 같은 특징을 가지고 있습니다.\n",
      "\n",
      "1. **AI 추론에 최적화**: Groq LPU는 AI 추론을 위해 설계되었습니다. 따라서 AI 모델을 실행하는 데 최적화되어 있습니다.\n",
      "2. **언어 처리에 최적화**: Groq LPU는 언어 처리를 위해 설계되었습니다. 따라서 언어 모델을 실행하는 데 최적화되어 있습니다.\n",
      "3. **에너지 효율성**: Groq LPU는 에너지 효율성을 높여 AI 추론을 더 빠르게 수행할 수 있도록 설계되었습니다.\n",
      "4. **가격 competitiveness**: Groq LPU는 가격이 저렴하여 AI 추론을 더 많은 사람들에게 접근할 수 있도록 설계되었습니다.\n",
      "\n",
      "이러한 특징들로 인해 Groq 엔진의 LLM 추론 속도가 빠르게 수행될 수 있습니다.\n",
      "Elapsed Time: 1.1311769485473633 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GROQ_API_KEY'] = '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' #기존 groq Key로 여러명이 동시에 동작시키면 차단될 수 있습니다. https://groq.com/ DEV CONSOLE에서 접속하여 여러분의 개별 groq Key를 넣어주세요.\n",
    "\n",
    "\n",
    "chat = ChatGroq(\n",
    "    temperature=0.1,\n",
    "    model=\"llama-3.2-11b-vision-preview\",\n",
    ")\n",
    "\n",
    "url = \"https://wow.groq.com/why-groq/\"\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "docs = loader.load()\n",
    "question = \"Groq 엔진의 LLM 추론 속도가 빠른 이유는 무엇인가요? 한국어로 답변하세요.\"\n",
    "\n",
    "\n",
    "system = \"Answer the question from given contexts. Answer in Korean.\"\n",
    "human = \"\"\"\n",
    "Context: {context}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat | StrOutputParser()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "result = chain.invoke(\n",
    "          {\"context\":docs[0].page_content,\n",
    "            'question':question})\n",
    "print(result)\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(f\"Elapsed Time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyM4Yt-fMpoX"
   },
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4F1k171MpoY"
   },
   "source": [
    "요약은 LLM의 아주 중요한 기능 중 하나입니다.   \n",
    "일반적으로, LLM의 Abstractive Summarization은 3개의 방법을 사용합니다.\n",
    "\n",
    "- Stuff : 전체 코퍼스를 하나의 프롬프트에 넣고 요약 생성하기\n",
    "- Map-Reduce : 코퍼스를 청크로 분리하고, 각 청크의 요약을 생성한 뒤 합치기\n",
    "- Refine: 코퍼스를 청크로 분리하고, 순차적으로 읽으며 요약 업데이트하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9vkYtYkMpoa"
   },
   "source": [
    "## Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUemn2D7ikKc"
   },
   "source": [
    "gemma 2 모델을 이용해 요약을 수행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kMmlgYzWMpoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Summarize the following paper in Korean.\\nEmphasize the uniqueness and contribution of the paper.\\nAnswer should be in 10 sentences.\\nPlease Answer in Korean.\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"\\n\\n [1706.03762] Attention Is All You Need\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\n\\n\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\\nDonate\\n\\n\\n\\n\\n\\n > cs > arXiv:1706.03762\\n  \\n\\n\\n\\n\\n\\nHelp | Advanced Search\\n\\n\\n\\n\\nAll fields\\nTitle\\nAuthor\\nAbstract\\nComments\\nJournal reference\\nACM classification\\nMSC classification\\nReport number\\narXiv identifier\\nDOI\\nORCID\\narXiv author ID\\nHelp pages\\nFull text\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nopen search\\n\\n\\n\\n\\n\\n\\nGO\\n\\n\\n\\nopen navigation menu\\n\\n\\nquick links\\n\\nLogin\\nHelp Pages\\nAbout\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComputer Science > Computation and Language\\n\\n\\narXiv:1706.03762 (cs)\\n    \\n\\n\\n\\n\\n  [Submitted on 12 Jun 2017 (v1), last revised 2 Aug 2023 (this version, v7)]\\nTitle:Attention Is All You Need\\nAuthors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin View a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\\nView PDF\\nHTML (experimental)\\n\\nAbstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n    \\n\\n\\n \\nComments:\\n15 pages, 5 figures\\n\\n\\nSubjects:\\n\\nComputation and Language (cs.CL); Machine Learning (cs.LG)\\n\\nCite as:\\narXiv:1706.03762 [cs.CL]\\n\\n\\n\\xa0\\n(or \\narXiv:1706.03762v7 [cs.CL] for this version)\\n          \\n\\n\\n\\xa0\\n https://doi.org/10.48550/arXiv.1706.03762\\n\\n\\nFocus to learn more\\n\\n\\n\\n                  arXiv-issued DOI via DataCite\\n\\n\\n\\n\\n\\n\\n\\nSubmission history From: Llion Jones [view email]       [v1]\\n        Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\\n[v2]\\n        Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\\n[v3]\\n        Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\\n[v4]\\n        Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\\n[v5]\\n        Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\\n[v6]\\n        Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\\n[v7]\\n        Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\\n\\n\\n\\n \\n\\nFull-text links:\\nAccess Paper:\\n\\n\\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authorsView PDFHTML (experimental)TeX SourceOther Formats\\nview license\\n\\n \\n    Current browse context: cs.CL\\n\\n\\n<\\xa0prev\\n\\n\\xa0 | \\xa0 \\nnext\\xa0>\\n\\n\\nnew\\n | \\nrecent\\n | 2017-06\\n\\n    Change to browse by:\\n    \\ncs\\ncs.LG\\n\\n\\n\\n\\nReferences & Citations\\n\\nNASA ADSGoogle Scholar\\nSemantic Scholar\\n\\n\\n\\n\\n\\n 123 blog links (what is this?)\\n        \\n\\n\\nDBLP - CS Bibliography\\n\\nlisting | bibtex \\n\\nAshish VaswaniNoam ShazeerNiki ParmarJakob UszkoreitLlion Jones …\\n\\n\\na\\nexport BibTeX citation\\nLoading...\\n\\n\\n\\n\\nBibTeX formatted citation\\n×\\n\\n\\nloading...\\n\\n\\nData provided by: \\n\\n\\n\\n\\nBookmark\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nBibliographic Tools\\n\\nBibliographic and Citation Tools\\n\\n\\n\\n\\n\\n\\nBibliographic Explorer Toggle\\n\\n\\n\\nBibliographic Explorer (What is the Explorer?)\\n\\n\\n\\n\\n\\n\\n\\nConnected Papers Toggle\\n\\n\\n\\nConnected Papers (What is Connected Papers?)\\n\\n\\n\\n\\n\\n\\nLitmaps Toggle\\n\\n\\n\\nLitmaps (What is Litmaps?)\\n\\n\\n\\n\\n\\n\\n\\nscite.ai Toggle\\n\\n\\n\\nscite Smart Citations (What are Smart Citations?)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode, Data, Media\\n\\nCode, Data and Media Associated with this Article\\n\\n\\n\\n\\n\\n\\nalphaXiv Toggle\\n\\n\\n\\nalphaXiv (What is alphaXiv?)\\n\\n\\n\\n\\n\\n\\n\\nLinks to Code Toggle\\n\\n\\n\\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\\n\\n\\n\\n\\n\\n\\n\\nDagsHub Toggle\\n\\n\\n\\nDagsHub (What is DagsHub?)\\n\\n\\n\\n\\n\\n\\n\\nGotitPub Toggle\\n\\n\\n\\nGotit.pub (What is GotitPub?)\\n\\n\\n\\n\\n\\n\\n\\nHuggingface Toggle\\n\\n\\n\\nHugging Face (What is Huggingface?)\\n\\n\\n\\n\\n\\n\\n\\nLinks to Code Toggle\\n\\n\\n\\nPapers with Code (What is Papers with Code?)\\n\\n\\n\\n\\n\\n\\n\\nScienceCast Toggle\\n\\n\\n\\nScienceCast (What is ScienceCast?)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDemos\\n\\nDemos\\n\\n\\n\\n\\n\\n\\nReplicate Toggle\\n\\n\\n\\nReplicate (What is Replicate?)\\n\\n\\n\\n\\n\\n\\n\\nSpaces Toggle\\n\\n\\n\\nHugging Face Spaces (What is Spaces?)\\n\\n\\n\\n\\n\\n\\n\\nSpaces Toggle\\n\\n\\n\\nTXYZ.AI (What is TXYZ.AI?)\\n\\n\\n\\n\\n\\n\\n\\n\\nRelated Papers\\n\\nRecommenders and Search Tools\\n\\n\\n\\n\\n\\n\\nLink to Influence Flower\\n\\n\\n\\nInfluence Flower (What are Influence Flowers?)\\n\\n\\n\\n\\n\\n\\n\\nCore recommender toggle\\n\\n\\n\\nCORE Recommender (What is CORE?)\\n\\n\\n\\n\\n\\nAuthor\\nVenue\\nInstitution\\nTopic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        About arXivLabs\\n      \\n\\n\\n\\narXivLabs: experimental projects with community collaborators\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhich authors of this paper are endorsers? |\\n    Disable MathJax (What is MathJax?)\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\nHelp\\n\\n\\n\\n\\n\\ncontact arXivClick here to contact arXiv\\n Contact\\n\\n\\nsubscribe to arXiv mailingsClick here to subscribe\\n Subscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright\\nPrivacy Policy\\n\\n\\n\\n\\nWeb Accessibility Assistance\\n\\n\\narXiv Operational Status \\n                    Get status notifications via\\n                    email\\n                    or slack\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stuff\n",
    "example_URL='https://arxiv.org/abs/1706.03762'\n",
    "\n",
    "loader = WebBaseLoader(example_URL)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "gemma2 = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"gemma2-9b-it\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''Summarize the following paper in Korean.\n",
    "Emphasize the uniqueness and contribution of the paper.\n",
    "Answer should be in 10 sentences.\n",
    "Please Answer in Korean.\n",
    "'''),\n",
    "    ('user', '''{text}''')])\n",
    "\n",
    "print(len(docs[0].page_content))\n",
    "summarize_prompt.format_messages(text=docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MOx1k71Mpoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 논문 \"Attention Is All You Need\"는 순환 신경망이나 합성곱 신경망과 같은 복잡한 구조를 사용하지 않고, **단순히 Attention 메커니즘만을 기반으로** 기존의 기계 번역 모델을 뛰어넘는 성능을 보여주는 새로운 네트워크 아키텍처인 Transformer를 제안합니다. \n",
      "\n",
      "이 논문의 가장 큰 혁신은 **RNN이나 CNN을 사용하지 않고도 효과적인 시퀀스 전환 모델을 구축할 수 있다는 것을 증명**한 것입니다. \n",
      "\n",
      "Transformer는 Attention 메커니즘을 통해 입력 시퀀스의 모든 요소 간의 관계를 효과적으로 학습하여, 기존 모델보다 더 높은 번역 품질을 달성합니다. \n",
      "\n",
      "실험 결과, Transformer는 WMT 2014 영어-독일어 및 영어-프랑스어 번역 작업에서 기존 최고 성능을 뛰어넘는 결과를 보였습니다. \n",
      "\n",
      "특히, Transformer는 **병렬 처리가 가능**하기 때문에 훈련 시간을 크게 단축시킬 수 있다는 장점도 가지고 있습니다. \n",
      "\n",
      "또한, Transformer는 **구문 분석과 같은 다른 NLP 작업에도 효과적으로 적용**될 수 있음을 보여주었습니다. \n",
      "\n",
      "이 논문은 기계 번역 분야에 큰 영향을 미쳤으며, 이후 다양한 NLP 모델에 Attention 메커니즘이 적용되는 계기가 되었습니다. \n",
      "\n",
      "Transformer는 **자연어 처리 분야의 새로운 지평을 열었으며, 이후 연구와 개발에 큰 영감을 주었습니다.**\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_chain = summarize_prompt | gemma2 | StrOutputParser() #chain 만들기\n",
    "summary = summarize_chain.invoke(docs[0].page_content)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCaANqOKMpoa"
   },
   "source": [
    "해당 방법은 매우 간단하지만, Context 길이를 넘어서는 경우 에러가 발생합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLO3VmBdikKd"
   },
   "source": [
    "## Map-Reduce\n",
    "LangChain의 PyMuPdfLoader를 이용하여 임의의 PDF를 요약해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "14r339tyikKd"
   },
   "outputs": [],
   "source": [
    "# # Password 있는 PDF 열기\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "path_material = './교재.pdf'\n",
    "\n",
    "# pypdf_loader = PyPDFLoader(path_material, password='비밀번호')\n",
    "# material_pages = pypdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BqWN9mrkikKd"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import urllib.request\n",
    "\n",
    "paper_URL = \"https://arxiv.org/pdf/2410.05983\"\n",
    "\n",
    "\n",
    "# 외부 링크에서 PDF 파일을 다운로드하는 코드\n",
    "urllib.request.urlretrieve(\n",
    "    paper_URL,\n",
    "    filename=\"paper.pdf\"\n",
    ")\n",
    "path = './paper.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ImhjYgkqikKd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97331"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "loader = PyMuPDFLoader(path)\n",
    "# 페이지별로 저장\n",
    "pages = loader.load()\n",
    "\n",
    "# 코퍼스에 모두 결합\n",
    "corpus = Document(page_content='')\n",
    "for page in pages:\n",
    "    corpus.page_content += page.page_content\n",
    "len(corpus.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvhDiGhdMpoa"
   },
   "source": [
    "긴 Context를 처리하기 위해, 전체 코퍼스를 작은 단위로 쪼개 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5pXmAy-Mpoa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000) #chunk_size는 한 chunk에 포함될 최대 글자수입니다.\n",
    "document_list = text_splitter.split_documents([corpus])\n",
    "len(document_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UE34hmK8Mpoa"
   },
   "source": [
    "이후 Map-Reduce와 Refine을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1rP6f8OMpoa"
   },
   "source": [
    "## Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "itfSciDVMpoa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:00<00:08,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 0\n",
      "본 논문은 장문맥 LLM(Long-Context LLMs)을 사용한 Retrieval-Augmented Generation(RAG) 시스템의 새로운 과제와 해결책을 제시합니다. 장문맥 LLM은 더 많은 정보를 처리할 수 있지만, 과도한 정보량은 LLM의 생성 성능 저하로 이어질 수 있습니다. 특히, 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM의 생성을 혼란스럽게 만들 수 있습니다. 이 논문에서는 장문맥 LLM을 사용한 RAG 시스템에서 hard negatives의 부정적인 영향을 분석하고, 이를 해결하기 위한 세 가지 새로운 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM이 더욱 관련성 있는 정보에 집중하도록 유도하는 방법입니다. 둘째, hard negatives에 대한 내성을 갖도록 LLM을 훈련하는 방법입니다. 셋째, LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 훈련하는 방법입니다. 이러한 방법들은 데이터 분포, 사용된 검색 알고리즘, 훈련 문맥 길이 등 다양한 요소를 고려하여 장문맥 LLM 기반 RAG 시스템의 성능을 향상시키는 데 기여합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:02<00:09,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 1\n",
      "본 논문은 장문맥 LLM(Long-Context LLMs)을 기반으로 한 RAG(Retrieval-Augmented Generation) 시스템에서 발생하는 과제들을 분석하고 해결 방안을 제시합니다. \n",
      "\n",
      "첫째, 장문맥 LLM이 처리할 수 있는 답변 텍스트의 길이가 증가함에 따라 RAG 성능이 향상되는지, 또는 오히려 감소하는지에 대한 질문을 던지며, 다양한 LLM과 검색기(retriever)를 사용하여 실험을 진행합니다. 실험 결과, 검색기의 성능이 높을수록 장문맥 LLM의 성능은 초기에는 증가하지만, 특정 범위를 넘어서면 감소하는 \"역U자형\" 패턴을 보이는 것으로 나타났습니다. 이는 검색기가 너무 많은 정보를 가져오면 LLM이 오히려 혼란스러워지고 성능이 저하될 수 있음을 시사합니다.\n",
      "\n",
      "둘째, 검색기의 성능과 LLM의 처리 능력 사이의 상호 작용을 분석합니다. 높은 재현율(recall)을 가진 검색기와 낮은 재현율을 가진 검색기 모두 사용하여 실험을 진행하고, LLM의 성능과 검색 결과의 재현율, 정확도(precision) 사이의 관계를 살펴봅니다. 결과적으로, 높은 재현율을 가진 검색기는 더 많은 정보를 가져오지만, 그 정보 중 일부가 오히려 LLM의 성능을 저하시키는 \"hard negatives\"로 작용하는 것으로 나타났습니다. 즉, 단순히 검색 결과의 양이 많을수록 LLM의 성능이 향상되는 것은 아니며, 검색 결과의 질과 LLM의 처리 능력 사이의 균형이 중요합니다.\n",
      "\n",
      "이러한 분석을 통해 논문은 장문맥 LLM을 활용한 RAG 시스템에서 발생하는 핵심 과제들을 명확히 제시하고, 이를 해결하기 위한 방향을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:03<00:08,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 2\n",
      "본 논문은 긴 텍스트 컨텍스트에서의 RAG(Retrieval Augmented Generation) 시스템에서 긴 텍스트를 처리하는 대규모 언어 모델(LLM)의 성능을 향상시키는 방법을 연구합니다. \n",
      "\n",
      "첫째, 논문은 기존 평가 지표인 정확도가 긴 텍스트 컨텍스트에서 LLM 성능을 완벽하게 반영하지 못한다는 점을 보여줍니다. 특히, \"hard negatives\" (관련성이 낮은 텍스트)가 LLM의 성능에 큰 영향을 미치며, 이를 정확도만으로는 측정하기 어렵다는 것을 강조합니다. \n",
      "\n",
      "둘째, 논문은 \"hard negatives\"가 LLM 성능에 미치는 영향을 규명하기 위해 다양한 retriever(정보 검색 알고리즘)와 LLM 조합을 사용한 실험을 수행합니다. 실험 결과, retriever의 성능이 높을수록 \"hard negatives\"의 영향이 더욱 커지는 것을 확인했습니다. \n",
      "\n",
      "셋째, 논문은 \"hard negatives\"의 문제를 해결하기 위해 \"retrieval reordering\" (정보 검색 결과의 순서 재배치)라는 새로운 방법을 제안합니다. 이 방법은 LLM이 \"lost-in-the-middle\" 현상(중간 정보에 대한 주의력 부족)을 극복하고, 가장 관련성이 높은 정보를 먼저 처리하도록 유도하여 \"hard negatives\"의 영향을 줄입니다. \n",
      "\n",
      "마지막으로, 논문은 \"data-augmented fine-tuning\" (데이터 증강을 통한 fine-tuning)을 통해 LLM의 \"hard negatives\"에 대한 내성을 향상시킬 수 있다는 가능성을 제시합니다. 이 방법은 LLM을 다양한 컨텍스트에서 학습시켜 \"hard negatives\"를 효과적으로 처리하도록 돕습니다. \n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [00:04<00:08,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 3\n",
      "본 논문은 Retrieval Augmented Generation(RAG)에 적합한 대규모 언어 모델(LLM)을 위한 데이터 증강 미세 조정 방법을 제시하고 그 효과를 분석합니다. \n",
      "\n",
      "첫째, 논문은 단순히 질문과 답변 쌍으로 미세 조정하는 것보다, 질문, 지시, 검색된 텍스트 패스지, 질문을 입력으로 받아 답변을 생성하는 RAG 특화 미세 조정을 통해 LLM의 견고성을 향상시킬 수 있다고 주장합니다. 이 방법은 다양한 검색된 텍스트를 학습시켜 LLM이 노이즈 속에서도 관련 정보를 효과적으로 식별하고 활용할 수 있도록 돕습니다. 실험 결과, RAG 특화 미세 조정은 검색된 텍스트의 수가 증가함에 따라 성능이 더욱 안정적으로 향상되는 것을 보여주며, 일반적인 질문-답변 미세 조정보다 우수한 성능을 보입니다.\n",
      "\n",
      "둘째, 논문은 LLM이 검색된 텍스트에서 관련 정보를 명확하게 식별하고 활용할 수 있도록, 미세 조정 과정에 중간 단계로 추론 단계를 추가하는 방법을 제안합니다. LLM은 질문과 검색된 텍스트를 입력으로 받아 관련 정보를 명시적으로 식별하는 추론 문단과 답변을 생성합니다. 이 방법은 LLM이 검색된 텍스트를 구조화된 방식으로 처리하고 관련 정보를 더욱 효과적으로 파악할 수 있도록 돕습니다. 실험 결과, 추론 단계를 추가한 미세 조정은 단순히 RAG 특화 미세 조정보다 더욱 향상된 성능을 보여줍니다.\n",
      "\n",
      "마지막으로, 논문은 RAG 특화 미세 조정의 효과를 높이기 위해 다양한 데이터 분포, 검색 알고리즘, 학습 텍스트 길이 등을 조사합니다. 다양한 데이터 분포를 학습시키면 LLM의 일반화 능력이 향상되며, 여러 검색 알고리즘을 사용하여 미세 조정하면 새로운 검색 알고리즘에서도 좋은 성능을 보입니다. 또한, 최대 텍스트 길이를 학습 범위로 설정하면 다양한 검색된 텍스트 수에 대해서도 안정적인 성능을 보입니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [00:05<00:06,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 4\n",
      "본 논문은 긴 텍스트 컨텍스트를 가진 대규모 언어 모델(LLM)이 검색 증강 생성(RAG) 시스템에서 장거리 입력에 대한 과제를 해결하는 방법을 연구합니다. 기존 연구와 달리, 본 논문은 많은 검색 결과를 사용하는 것이 성능 향상에 도움이 되지 않고 오히려 성능 저하를 초래할 수 있다는 것을 발견했습니다. 이는 검색된 \"hard negatives\"가 LLM의 성능에 부정적인 영향을 미치기 때문입니다. 이 문제를 해결하기 위해, 본 논문에서는 학습 전 검색 결과 재정렬, RAG 특화 암묵적 LLM 미세 조정, 그리고 중간 추론을 포함한 RAG 지향적 LLM 미세 조정 등 세 가지 해결책을 제안하고 평가합니다. 또한, 데이터 분포, 학습에 사용되는 검색 엔진, 그리고 학습 컨텍스트 길이와 같은 요소들이 RAG 성능에 미치는 영향을 분석합니다. \n",
      "\n",
      "본 논문은 다음과 같은 주요 내용을 다룹니다.\n",
      "\n",
      "* 많은 검색 결과를 사용하는 것이 LLM 성능을 향상시키지 않고 오히려 저하시킬 수 있다는 것을 발견했습니다.\n",
      "* 이러한 성능 저하의 원인은 검색된 \"hard negatives\" 때문임을 밝혔습니다.\n",
      "* 학습 전 검색 결과 재정렬, RAG 특화 암묵적 LLM 미세 조정, 그리고 중간 추론을 포함한 RAG 지향적 LLM 미세 조정 등 세 가지 해결책을 제안했습니다.\n",
      "* 데이터 분포, 학습에 사용되는 검색 엔진, 그리고 학습 컨텍스트 길이와 같은 요소들이 RAG 성능에 미치는 영향을 분석했습니다.\n",
      "* 미래 연구 방향으로는 더욱 고급화된 검색 결과 정렬 방법을 사용한 자동화된 위치 최적화와 다단계 추론 체인을 위한 LLM 미세 조정을 제시했습니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 6/11 [00:19<00:27,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 5\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)이 Retrieval Augmented Generation(RAG)과 같은 기존 기법과 어떻게 상호 작용하는지, 그리고 장문 입력에 대한 RAG의 새로운 가능성을 탐구합니다. \n",
      "\n",
      "첫째, 연구자들은 장문 맥락 LLM이 RAG에 적용될 때 발생하는 새로운 과제들을 분석합니다. 특히, 장문 맥락 LLM은 많은 양의 정보를 처리해야 하기 때문에, 검색된 텍스트의 양과 질이 RAG 성능에 큰 영향을 미칠 수 있습니다. \n",
      "\n",
      "둘째, 연구자들은 다양한 LLM과 검색 알고리즘을 사용하여 NQ와 PopQA와 같은 데이터셋에서 실험을 수행합니다. 실험 결과, 강력한 검색 알고리즘(e5)을 사용할 때 LLM은 초기에는 성능이 향상되지만, 검색된 텍스트의 양이 증가함에 따라 성능이 감소하는 경향을 보입니다. 반면, 약한 검색 알고리즘(BM25)을 사용할 때는 검색된 텍스트의 양이 증가함에 따라 성능이 지속적으로 향상됩니다. \n",
      "\n",
      "셋째, 연구자들은 LLM이 \"hard negative\" (정답이 없는 관련성 있는 텍스트)에 취약하다는 것을 발견합니다. 특히, 강력한 검색 알고리즘으로 검색된 \"hard negative\"는 LLM의 성능에 더 큰 영향을 미칩니다. \n",
      "\n",
      "넷째, 연구자들은 장문 맥락 LLM의 RAG 성능을 향상시키기 위한 몇 가지 방법을 제안합니다. 예를 들어, \"hard negative\"를 제거하거나, 검색된 텍스트의 질을 향상시키는 방법을 고려할 수 있습니다. \n",
      "\n",
      "마지막으로, 연구자들은 장문 맥락 LLM과 RAG의 잠재력을 강조하며, 앞으로 이 분야에서 더 많은 연구가 필요하다고 주장합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [00:34<00:34,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 6\n",
      "본 논문은 장문 텍스트 입력에 대한 Retrieval Augmented Generation(RAG) 시스템의 성능을 향상시키기 위한 방법을 제시합니다. 특히, 장문 텍스트를 효과적으로 처리하고 이해하기 위해 Long-Context LLMs(대용량 언어 모델)을 RAG에 적용하는 방법에 초점을 맞춥니다. \n",
      "\n",
      "논문은 첫째, 장문 텍스트 입력에 대한 RAG 시스템의 기존 문제점을 분석합니다. 둘째, 이러한 문제점을 해결하기 위해 Long-Context LLMs을 활용한 새로운 RAG 프레임워크를 제안합니다. \n",
      "\n",
      "셋째, 제안된 프레임워크가 다양한 데이터셋에서 기존 RAG 시스템보다 우수한 성능을 보여준다는 것을 실험을 통해 입증합니다. 넷째, Long-Context LLMs의 장점을 활용하여 질문에 대한 답변을 생성하는 과정에서 더욱 정확하고 관련성 있는 결과를 얻을 수 있다는 것을 보여줍니다. 다섯째, 논문은 Hard Negative 문제를 다루며, LLMs가 답변과 관련이 없는 텍스트에 영향을 받지 않도록 하는 방법을 제시합니다. 마지막으로, Long-Context LLMs를 RAG에 적용함으로써 정보 검색 및 질문 답변 시스템의 성능을 향상시킬 수 있다는 결론을 내립니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 8/11 [00:45<00:28,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 7\n",
      "본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation)의 새로운 접근 방식을 제시하며, 특히 긴 텍스트 컨텍스트에서의 성능 향상에 초점을 맞춥니다. \n",
      "\n",
      "첫째, 논문은 긴 텍스트 컨텍스트에서의 RAG의 어려움을 분석하고, 이를 극복하기 위한 다양한 방법을 제안합니다. 둘째, 논문은 긴 입력에 대한 RAG 모델의 성능을 향상시키기 위해 새로운 데이터셋과 훈련 방법을 소개합니다. 셋째, 논문은 제안된 방법이 기존 방법에 비해 뛰어난 성능을 보여준다는 것을 실험 결과를 통해 입증합니다. 넷째, 논문은 긴 텍스트 컨텍스트에서의 RAG 모델의 성능 향상을 위한 추가적인 연구 방향을 제시합니다. 다섯째, 논문은 긴 입력에 대한 RAG 모델의 성능 향상을 위한 실용적인 방법을 제공합니다. 마지막으로, 논문은 긴 텍스트 컨텍스트에서의 RAG 모델의 성능 향상에 대한 심층적인 분석을 제공합니다. \n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [00:57<00:20, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 8\n",
      "본 논문은 Long-Context LLMs를 활용한 RAG(Retrieval Augmented Generation) 모델의 성능 향상을 다룹니다. 특히, 중간 단계의 추론 능력을 강화하여 질문에 대한 답변을 생성하는 데 초점을 맞춥니다. \n",
      "\n",
      "본 논문은 다양한 데이터셋(NQ, Wizard of Wikipedia, FEVER, MMLU)을 사용하여 RAG 모델을 학습하고 평가합니다. 학습 과정에서, 모델은 질문과 관련된 문서를 분석하고, 답변을 생성하기 위한 논리적 근거를 제시하는 능력을 향상시키도록 지도 학습됩니다. \n",
      "\n",
      "평가 결과, 중간 단계 추론을 활용한 RAG 모델은 기존의 RAG 모델보다 질문에 대한 답변의 정확도와 완전성을 향상시키는 것을 보여줍니다. \n",
      "\n",
      "특히, 긴 텍스트 입력에 대한 처리 능력이 향상되었으며, 복잡한 질문에 대한 답변을 생성하는 데에도 효과적임을 확인했습니다. \n",
      "\n",
      "논문은 또한 데이터 증강 기법을 활용하여 RAG 모델의 성능을 더욱 향상시킬 수 있는 가능성을 제시합니다. \n",
      "\n",
      "결론적으로, 본 논문은 Long-Context LLMs를 활용한 RAG 모델의 성능 향상을 위한 새로운 접근 방식을 제시하며, 긴 텍스트 입력 처리 및 복잡한 질문 답변에 대한 잠재력을 보여줍니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [01:08<00:10, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 9\n",
      "본 논문은 Gemma-2-9B와 같은 대규모 언어 모델(LLM)을 사용하여 질의응답(RAG) 시스템을 개선하는 방법에 대해 다룹니다. 특히, 데이터 증강된 RAG fine-tuning 기법이 LLM의 성능을 향상시키는 데 효과적임을 보여줍니다. \n",
      "\n",
      "첫째, 논문은 다양한 데이터셋에서 Gemma-2-9B 모델을 RAG fine-tuning하여 그 성능을 평가합니다. RAG fine-tuning은 일반적인 챗 모델보다 질의응답 작업에서 더 높은 정확도를 보입니다. 둘째, 논문은 RAG fine-tuning에 중간 단계의 추론을 추가하여 모델의 성능을 더욱 향상시키는 방법을 제시합니다. 이 방법은 복잡한 질문에 대한 답변을 생성하는 데 유용합니다. 셋째, 논문은 Mistral-Nemo-12B 모델과 같은 다른 LLM에서도 데이터 증강된 RAG fine-tuning 기법이 효과적임을 보여줍니다. 넷째, 논문은 RAG fine-tuning이 LLM의 장점을 활용하여 질의응답 시스템의 성능을 향상시키는 데 유용한 방법임을 강조합니다. 마지막으로, 논문은 앞으로의 연구 방향으로 데이터 증강 기법의 다양한 적용 및 더욱 복잡한 질문에 대한 답변을 생성하는 방법에 대한 연구를 제안합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:21<00:00,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 10\n",
      "본 논문은 긴 텍스트 입력에 대한 RAG(Retrieval Augmented Generation)의 효율성을 높이기 위해 Long-Context LLM(Large Language Model)을 사용하는 방법을 연구합니다. \n",
      "\n",
      "첫째, Mistral-Nemo-12B와 Gemini-1.0-Pro 모델을 사용하여 RAG-specific fine-tuning의 효과를 보여줍니다. \n",
      "\n",
      "둘째, RAG-specific fine-tuning 데이터의 크기가 LLM 성능에 미치는 영향을 분석하여 데이터 규모가 증가함에 따라 성능이 향상됨을 확인합니다. \n",
      "\n",
      "셋째, 일반적인 SFT(Supervised Fine-Tuning) 데이터와 RAG-specific 데이터를 결합하여 LLM을 학습시키고, 이 방법이 RAG 성능을 향상시키면서도 일반적인 언어 처리 능력을 유지할 수 있음을 보여줍니다. \n",
      "\n",
      "결론적으로, 본 논문은 긴 텍스트 입력을 처리하는 RAG 시스템에서 Long-Context LLM을 활용하고, RAG-specific fine-tuning과 데이터 규모 조절을 통해 성능을 향상시킬 수 있는 방법을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Map 과정 : 각 문서에 대해 요약을 생성합니다. -> Long Context 인 경우, 균일하게 분할하여 요약하고 다시 합치는 방법으로 성능 개선\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''주어진 논문의 내용을 읽고 한국어로 요약하세요.\n",
    "요약은 1개의 문단과 문단별 6개의 문장으로 작성하세요.\n",
    "답변은 한국어로 작성하세요.\n",
    "같은 내용을 반복하지 마세요.\n",
    "'''),\n",
    "    ('user', '''{text}''')])\n",
    "\n",
    "map_chain  = map_prompt | gemma2 | StrOutputParser()\n",
    "\n",
    "raw_summaries = []\n",
    "for i in tqdm(range(len(document_list))):\n",
    "    response = map_chain.invoke(document_list[i].page_content)\n",
    "    raw_summaries.append(response)\n",
    "    print('')\n",
    "    print('#',i)\n",
    "    print(response)\n",
    "    print('===========================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['본 논문은 장문맥 LLM(Long-Context LLMs)을 사용한 Retrieval-Augmented Generation(RAG) 시스템의 새로운 과제와 해결책을 제시합니다. 장문맥 LLM은 더 많은 정보를 처리할 수 있지만, 과도한 정보량은 LLM의 생성 성능 저하로 이어질 수 있습니다. 특히, 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM의 생성을 혼란스럽게 만들 수 있습니다. 이 논문에서는 장문맥 LLM을 사용한 RAG 시스템에서 hard negatives의 부정적인 영향을 분석하고, 이를 해결하기 위한 세 가지 새로운 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM이 더욱 관련성 있는 정보에 집중하도록 유도하는 방법입니다. 둘째, hard negatives에 대한 내성을 갖도록 LLM을 훈련하는 방법입니다. 셋째, LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 훈련하는 방법입니다. 이러한 방법들은 데이터 분포, 사용된 검색 알고리즘, 훈련 문맥 길이 등 다양한 요소를 고려하여 장문맥 LLM 기반 RAG 시스템의 성능을 향상시키는 데 기여합니다.\\n\\n\\n',\n",
       " '본 논문은 장문맥 LLM(Long-Context LLMs)을 기반으로 한 RAG(Retrieval-Augmented Generation) 시스템에서 발생하는 과제들을 분석하고 해결 방안을 제시합니다. \\n\\n첫째, 장문맥 LLM이 처리할 수 있는 답변 텍스트의 길이가 증가함에 따라 RAG 성능이 향상되는지, 또는 오히려 감소하는지에 대한 질문을 던지며, 다양한 LLM과 검색기(retriever)를 사용하여 실험을 진행합니다. 실험 결과, 검색기의 성능이 높을수록 장문맥 LLM의 성능은 초기에는 증가하지만, 특정 범위를 넘어서면 감소하는 \"역U자형\" 패턴을 보이는 것으로 나타났습니다. 이는 검색기가 너무 많은 정보를 가져오면 LLM이 오히려 혼란스러워지고 성능이 저하될 수 있음을 시사합니다.\\n\\n둘째, 검색기의 성능과 LLM의 처리 능력 사이의 상호 작용을 분석합니다. 높은 재현율(recall)을 가진 검색기와 낮은 재현율을 가진 검색기 모두 사용하여 실험을 진행하고, LLM의 성능과 검색 결과의 재현율, 정확도(precision) 사이의 관계를 살펴봅니다. 결과적으로, 높은 재현율을 가진 검색기는 더 많은 정보를 가져오지만, 그 정보 중 일부가 오히려 LLM의 성능을 저하시키는 \"hard negatives\"로 작용하는 것으로 나타났습니다. 즉, 단순히 검색 결과의 양이 많을수록 LLM의 성능이 향상되는 것은 아니며, 검색 결과의 질과 LLM의 처리 능력 사이의 균형이 중요합니다.\\n\\n이러한 분석을 통해 논문은 장문맥 LLM을 활용한 RAG 시스템에서 발생하는 핵심 과제들을 명확히 제시하고, 이를 해결하기 위한 방향을 제시합니다. \\n\\n\\n',\n",
       " '본 논문은 긴 텍스트 컨텍스트에서의 RAG(Retrieval Augmented Generation) 시스템에서 긴 텍스트를 처리하는 대규모 언어 모델(LLM)의 성능을 향상시키는 방법을 연구합니다. \\n\\n첫째, 논문은 기존 평가 지표인 정확도가 긴 텍스트 컨텍스트에서 LLM 성능을 완벽하게 반영하지 못한다는 점을 보여줍니다. 특히, \"hard negatives\" (관련성이 낮은 텍스트)가 LLM의 성능에 큰 영향을 미치며, 이를 정확도만으로는 측정하기 어렵다는 것을 강조합니다. \\n\\n둘째, 논문은 \"hard negatives\"가 LLM 성능에 미치는 영향을 규명하기 위해 다양한 retriever(정보 검색 알고리즘)와 LLM 조합을 사용한 실험을 수행합니다. 실험 결과, retriever의 성능이 높을수록 \"hard negatives\"의 영향이 더욱 커지는 것을 확인했습니다. \\n\\n셋째, 논문은 \"hard negatives\"의 문제를 해결하기 위해 \"retrieval reordering\" (정보 검색 결과의 순서 재배치)라는 새로운 방법을 제안합니다. 이 방법은 LLM이 \"lost-in-the-middle\" 현상(중간 정보에 대한 주의력 부족)을 극복하고, 가장 관련성이 높은 정보를 먼저 처리하도록 유도하여 \"hard negatives\"의 영향을 줄입니다. \\n\\n마지막으로, 논문은 \"data-augmented fine-tuning\" (데이터 증강을 통한 fine-tuning)을 통해 LLM의 \"hard negatives\"에 대한 내성을 향상시킬 수 있다는 가능성을 제시합니다. 이 방법은 LLM을 다양한 컨텍스트에서 학습시켜 \"hard negatives\"를 효과적으로 처리하도록 돕습니다. \\n\\n\\n',\n",
       " '본 논문은 Retrieval Augmented Generation(RAG)에 적합한 대규모 언어 모델(LLM)을 위한 데이터 증강 미세 조정 방법을 제시하고 그 효과를 분석합니다. \\n\\n첫째, 논문은 단순히 질문과 답변 쌍으로 미세 조정하는 것보다, 질문, 지시, 검색된 텍스트 패스지, 질문을 입력으로 받아 답변을 생성하는 RAG 특화 미세 조정을 통해 LLM의 견고성을 향상시킬 수 있다고 주장합니다. 이 방법은 다양한 검색된 텍스트를 학습시켜 LLM이 노이즈 속에서도 관련 정보를 효과적으로 식별하고 활용할 수 있도록 돕습니다. 실험 결과, RAG 특화 미세 조정은 검색된 텍스트의 수가 증가함에 따라 성능이 더욱 안정적으로 향상되는 것을 보여주며, 일반적인 질문-답변 미세 조정보다 우수한 성능을 보입니다.\\n\\n둘째, 논문은 LLM이 검색된 텍스트에서 관련 정보를 명확하게 식별하고 활용할 수 있도록, 미세 조정 과정에 중간 단계로 추론 단계를 추가하는 방법을 제안합니다. LLM은 질문과 검색된 텍스트를 입력으로 받아 관련 정보를 명시적으로 식별하는 추론 문단과 답변을 생성합니다. 이 방법은 LLM이 검색된 텍스트를 구조화된 방식으로 처리하고 관련 정보를 더욱 효과적으로 파악할 수 있도록 돕습니다. 실험 결과, 추론 단계를 추가한 미세 조정은 단순히 RAG 특화 미세 조정보다 더욱 향상된 성능을 보여줍니다.\\n\\n마지막으로, 논문은 RAG 특화 미세 조정의 효과를 높이기 위해 다양한 데이터 분포, 검색 알고리즘, 학습 텍스트 길이 등을 조사합니다. 다양한 데이터 분포를 학습시키면 LLM의 일반화 능력이 향상되며, 여러 검색 알고리즘을 사용하여 미세 조정하면 새로운 검색 알고리즘에서도 좋은 성능을 보입니다. 또한, 최대 텍스트 길이를 학습 범위로 설정하면 다양한 검색된 텍스트 수에 대해서도 안정적인 성능을 보입니다.\\n\\n\\n',\n",
       " '본 논문은 긴 텍스트 컨텍스트를 가진 대규모 언어 모델(LLM)이 검색 증강 생성(RAG) 시스템에서 장거리 입력에 대한 과제를 해결하는 방법을 연구합니다. 기존 연구와 달리, 본 논문은 많은 검색 결과를 사용하는 것이 성능 향상에 도움이 되지 않고 오히려 성능 저하를 초래할 수 있다는 것을 발견했습니다. 이는 검색된 \"hard negatives\"가 LLM의 성능에 부정적인 영향을 미치기 때문입니다. 이 문제를 해결하기 위해, 본 논문에서는 학습 전 검색 결과 재정렬, RAG 특화 암묵적 LLM 미세 조정, 그리고 중간 추론을 포함한 RAG 지향적 LLM 미세 조정 등 세 가지 해결책을 제안하고 평가합니다. 또한, 데이터 분포, 학습에 사용되는 검색 엔진, 그리고 학습 컨텍스트 길이와 같은 요소들이 RAG 성능에 미치는 영향을 분석합니다. \\n\\n본 논문은 다음과 같은 주요 내용을 다룹니다.\\n\\n* 많은 검색 결과를 사용하는 것이 LLM 성능을 향상시키지 않고 오히려 저하시킬 수 있다는 것을 발견했습니다.\\n* 이러한 성능 저하의 원인은 검색된 \"hard negatives\" 때문임을 밝혔습니다.\\n* 학습 전 검색 결과 재정렬, RAG 특화 암묵적 LLM 미세 조정, 그리고 중간 추론을 포함한 RAG 지향적 LLM 미세 조정 등 세 가지 해결책을 제안했습니다.\\n* 데이터 분포, 학습에 사용되는 검색 엔진, 그리고 학습 컨텍스트 길이와 같은 요소들이 RAG 성능에 미치는 영향을 분석했습니다.\\n* 미래 연구 방향으로는 더욱 고급화된 검색 결과 정렬 방법을 사용한 자동화된 위치 최적화와 다단계 추론 체인을 위한 LLM 미세 조정을 제시했습니다.\\n\\n\\n\\n',\n",
       " '본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)이 Retrieval Augmented Generation(RAG)과 같은 기존 기법과 어떻게 상호 작용하는지, 그리고 장문 입력에 대한 RAG의 새로운 가능성을 탐구합니다. \\n\\n첫째, 연구자들은 장문 맥락 LLM이 RAG에 적용될 때 발생하는 새로운 과제들을 분석합니다. 특히, 장문 맥락 LLM은 많은 양의 정보를 처리해야 하기 때문에, 검색된 텍스트의 양과 질이 RAG 성능에 큰 영향을 미칠 수 있습니다. \\n\\n둘째, 연구자들은 다양한 LLM과 검색 알고리즘을 사용하여 NQ와 PopQA와 같은 데이터셋에서 실험을 수행합니다. 실험 결과, 강력한 검색 알고리즘(e5)을 사용할 때 LLM은 초기에는 성능이 향상되지만, 검색된 텍스트의 양이 증가함에 따라 성능이 감소하는 경향을 보입니다. 반면, 약한 검색 알고리즘(BM25)을 사용할 때는 검색된 텍스트의 양이 증가함에 따라 성능이 지속적으로 향상됩니다. \\n\\n셋째, 연구자들은 LLM이 \"hard negative\" (정답이 없는 관련성 있는 텍스트)에 취약하다는 것을 발견합니다. 특히, 강력한 검색 알고리즘으로 검색된 \"hard negative\"는 LLM의 성능에 더 큰 영향을 미칩니다. \\n\\n넷째, 연구자들은 장문 맥락 LLM의 RAG 성능을 향상시키기 위한 몇 가지 방법을 제안합니다. 예를 들어, \"hard negative\"를 제거하거나, 검색된 텍스트의 질을 향상시키는 방법을 고려할 수 있습니다. \\n\\n마지막으로, 연구자들은 장문 맥락 LLM과 RAG의 잠재력을 강조하며, 앞으로 이 분야에서 더 많은 연구가 필요하다고 주장합니다.\\n\\n\\n',\n",
       " '본 논문은 장문 텍스트 입력에 대한 Retrieval Augmented Generation(RAG) 시스템의 성능을 향상시키기 위한 방법을 제시합니다. 특히, 장문 텍스트를 효과적으로 처리하고 이해하기 위해 Long-Context LLMs(대용량 언어 모델)을 RAG에 적용하는 방법에 초점을 맞춥니다. \\n\\n논문은 첫째, 장문 텍스트 입력에 대한 RAG 시스템의 기존 문제점을 분석합니다. 둘째, 이러한 문제점을 해결하기 위해 Long-Context LLMs을 활용한 새로운 RAG 프레임워크를 제안합니다. \\n\\n셋째, 제안된 프레임워크가 다양한 데이터셋에서 기존 RAG 시스템보다 우수한 성능을 보여준다는 것을 실험을 통해 입증합니다. 넷째, Long-Context LLMs의 장점을 활용하여 질문에 대한 답변을 생성하는 과정에서 더욱 정확하고 관련성 있는 결과를 얻을 수 있다는 것을 보여줍니다. 다섯째, 논문은 Hard Negative 문제를 다루며, LLMs가 답변과 관련이 없는 텍스트에 영향을 받지 않도록 하는 방법을 제시합니다. 마지막으로, Long-Context LLMs를 RAG에 적용함으로써 정보 검색 및 질문 답변 시스템의 성능을 향상시킬 수 있다는 결론을 내립니다.\\n\\n\\n',\n",
       " '본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation)의 새로운 접근 방식을 제시하며, 특히 긴 텍스트 컨텍스트에서의 성능 향상에 초점을 맞춥니다. \\n\\n첫째, 논문은 긴 텍스트 컨텍스트에서의 RAG의 어려움을 분석하고, 이를 극복하기 위한 다양한 방법을 제안합니다. 둘째, 논문은 긴 입력에 대한 RAG 모델의 성능을 향상시키기 위해 새로운 데이터셋과 훈련 방법을 소개합니다. 셋째, 논문은 제안된 방법이 기존 방법에 비해 뛰어난 성능을 보여준다는 것을 실험 결과를 통해 입증합니다. 넷째, 논문은 긴 텍스트 컨텍스트에서의 RAG 모델의 성능 향상을 위한 추가적인 연구 방향을 제시합니다. 다섯째, 논문은 긴 입력에 대한 RAG 모델의 성능 향상을 위한 실용적인 방법을 제공합니다. 마지막으로, 논문은 긴 텍스트 컨텍스트에서의 RAG 모델의 성능 향상에 대한 심층적인 분석을 제공합니다. \\n\\n\\n',\n",
       " '본 논문은 Long-Context LLMs를 활용한 RAG(Retrieval Augmented Generation) 모델의 성능 향상을 다룹니다. 특히, 중간 단계의 추론 능력을 강화하여 질문에 대한 답변을 생성하는 데 초점을 맞춥니다. \\n\\n본 논문은 다양한 데이터셋(NQ, Wizard of Wikipedia, FEVER, MMLU)을 사용하여 RAG 모델을 학습하고 평가합니다. 학습 과정에서, 모델은 질문과 관련된 문서를 분석하고, 답변을 생성하기 위한 논리적 근거를 제시하는 능력을 향상시키도록 지도 학습됩니다. \\n\\n평가 결과, 중간 단계 추론을 활용한 RAG 모델은 기존의 RAG 모델보다 질문에 대한 답변의 정확도와 완전성을 향상시키는 것을 보여줍니다. \\n\\n특히, 긴 텍스트 입력에 대한 처리 능력이 향상되었으며, 복잡한 질문에 대한 답변을 생성하는 데에도 효과적임을 확인했습니다. \\n\\n논문은 또한 데이터 증강 기법을 활용하여 RAG 모델의 성능을 더욱 향상시킬 수 있는 가능성을 제시합니다. \\n\\n결론적으로, 본 논문은 Long-Context LLMs를 활용한 RAG 모델의 성능 향상을 위한 새로운 접근 방식을 제시하며, 긴 텍스트 입력 처리 및 복잡한 질문 답변에 대한 잠재력을 보여줍니다.\\n\\n\\n',\n",
       " '본 논문은 Gemma-2-9B와 같은 대규모 언어 모델(LLM)을 사용하여 질의응답(RAG) 시스템을 개선하는 방법에 대해 다룹니다. 특히, 데이터 증강된 RAG fine-tuning 기법이 LLM의 성능을 향상시키는 데 효과적임을 보여줍니다. \\n\\n첫째, 논문은 다양한 데이터셋에서 Gemma-2-9B 모델을 RAG fine-tuning하여 그 성능을 평가합니다. RAG fine-tuning은 일반적인 챗 모델보다 질의응답 작업에서 더 높은 정확도를 보입니다. 둘째, 논문은 RAG fine-tuning에 중간 단계의 추론을 추가하여 모델의 성능을 더욱 향상시키는 방법을 제시합니다. 이 방법은 복잡한 질문에 대한 답변을 생성하는 데 유용합니다. 셋째, 논문은 Mistral-Nemo-12B 모델과 같은 다른 LLM에서도 데이터 증강된 RAG fine-tuning 기법이 효과적임을 보여줍니다. 넷째, 논문은 RAG fine-tuning이 LLM의 장점을 활용하여 질의응답 시스템의 성능을 향상시키는 데 유용한 방법임을 강조합니다. 마지막으로, 논문은 앞으로의 연구 방향으로 데이터 증강 기법의 다양한 적용 및 더욱 복잡한 질문에 대한 답변을 생성하는 방법에 대한 연구를 제안합니다.\\n\\n\\n',\n",
       " '본 논문은 긴 텍스트 입력에 대한 RAG(Retrieval Augmented Generation)의 효율성을 높이기 위해 Long-Context LLM(Large Language Model)을 사용하는 방법을 연구합니다. \\n\\n첫째, Mistral-Nemo-12B와 Gemini-1.0-Pro 모델을 사용하여 RAG-specific fine-tuning의 효과를 보여줍니다. \\n\\n둘째, RAG-specific fine-tuning 데이터의 크기가 LLM 성능에 미치는 영향을 분석하여 데이터 규모가 증가함에 따라 성능이 향상됨을 확인합니다. \\n\\n셋째, 일반적인 SFT(Supervised Fine-Tuning) 데이터와 RAG-specific 데이터를 결합하여 LLM을 학습시키고, 이 방법이 RAG 성능을 향상시키면서도 일반적인 언어 처리 능력을 유지할 수 있음을 보여줍니다. \\n\\n결론적으로, 본 논문은 긴 텍스트 입력을 처리하는 RAG 시스템에서 Long-Context LLM을 활용하고, RAG-specific fine-tuning과 데이터 규모 조절을 통해 성능을 향상시킬 수 있는 방법을 제시합니다. \\n\\n\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ckS-TZVsMpoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 장문 맥락 LLM을 활용한 RAG 시스템 성능 향상 방안 연구\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용하여 성능을 향상시키는 방법을 연구합니다. \n",
      "\n",
      "**핵심 내용:**\n",
      "\n",
      "* **장문 맥락 LLM의 RAG 적용 문제점:** 장문 맥락 LLM은 많은 정보를 처리해야 하기 때문에, 검색된 텍스트의 양과 질이 RAG 성능에 큰 영향을 미칩니다. 특히, 강력한 검색 알고리즘으로 검색된 \"hard negatives\" (정답과 관련 없는 텍스트)는 LLM의 성능을 저하시킬 수 있습니다.\n",
      "* **해결 방안:** \n",
      "    * 검색된 문서의 순서를 재정렬하여 LLM이 관련성 높은 정보에 집중하도록 유도\n",
      "    * hard negatives에 대한 내성을 갖도록 LLM을 훈련\n",
      "    * LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 훈련\n",
      "* **실험 결과:** 제안된 방법들은 다양한 데이터셋과 검색 알고리즘에서 성능 향상을 보여주며, 특히 긴 텍스트 입력에 대한 처리 능력을 향상시키는 데 효과적입니다.\n",
      "\n",
      "**결론:**\n",
      "\n",
      "장문 맥락 LLM을 RAG 시스템에 적용하면 긴 텍스트 입력 처리 능력을 향상시킬 수 있으며, 데이터 분포, 사용된 검색 알고리즘, 훈련 텍스트 길이 등 다양한 요소를 고려하여 성능을 극대화할 수 있습니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reduce 과정 : 각 문서의 요약을 하나로 합칩니다.\n",
    "reduce_prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    ('system', '''\n",
    "Generate a summary of the following text that includes the following elements:\n",
    "\n",
    "* A title that accurately reflects the content of the text.\n",
    "* An introduction paragraph that provides an overview of the topic.\n",
    "* Bullet points that list the key points of the text.\n",
    "* A conclusion paragraph that summarizes the main points of the text.\n",
    "\n",
    "Answer in Korean.\n",
    "'''),\n",
    "    ('user', '''{text}\n",
    "---\n",
    "요약(In Korean):\n",
    "''')])\n",
    "\n",
    "reduce_chain = reduce_prompt | gemma2 | StrOutputParser()\n",
    "\n",
    "summary = reduce_chain.invoke('\\n---\\n'.join(raw_summaries))\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi84F2cYMpoa"
   },
   "source": [
    "## Refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZfANrVtMpoa"
   },
   "source": [
    "Refine은 청크를 순서대로 참고하며, 매 시점 요약문을 작성합니다.   \n",
    "요약문과 새로운 청크를 비교하여, 요약문을 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7dee-KnHMpob"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. \n",
      "\n",
      "장문 맥락 LLM은 더 많은 정보를 활용할 수 있어 성능 향상을 기대할 수 있지만, 실험 결과는 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. \n",
      "\n",
      "이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "결론적으로, 본 논문은 장문 맥락 LLM을 사용하는 RAG 시스템에서 발생하는 새로운 문제점을 분석하고, 이를 해결하기 위한 효과적인 방법을 제시합니다.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 청크 순서대로 각 청크들을 요약하고, 앞부분 청크 요약과 현재 청크를 연결하여 요약을 업데이트하며 진행한다.\n",
    "# 시간은 오래 걸릴 수 있으나, 청크 업데이트가 잦은 경우에 사용\n",
    "first_summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''당신은 인공지능 전문가입니다.\n",
    "주어진 논문의 내용을 읽고 한국어로 요약하세요.\n",
    "요약은 1개의 문단과 문단별 5개의 문장으로 작성하세요.\n",
    "답변은 한국어로 작성하세요.'''),\n",
    "    ('user', '''{text}''')])\n",
    "\n",
    "\n",
    "\n",
    "X = first_summarize_prompt.format_messages(text=document_list[0])\n",
    "\n",
    "intermediate_summary = gemma2.invoke(X).content\n",
    "print(intermediate_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bBpA1i76Mpob"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:08,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있어 성능 향상을 기대할 수 있지만, 실험 결과는 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. \n",
      "\n",
      "특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도, 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. 이는 단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다.\n",
      "\n",
      "이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:09<00:42,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있어 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도, 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "새로운 내용에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:24<01:09,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있어 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "새로운 내용에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "본 논문에서는 RAG-specific fine-tuning 방법을 통해 LLM의 hard negatives에 대한 내성을 높이고, 관련성 있는 정보를 명확히 식별하도록 학습시키는 방법을 제안합니다. 이를 통해 LLM의 RAG 성능을 향상시킬 수 있습니다. 또한, 다양한 데이터 분포와 검색 알고리즘의 영향을 분석하여 RAG fine-tuning의 효과적인 전략을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:42<01:18, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있어 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "새로운 내용에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "본 논문에서는 RAG-specific fine-tuning 방법을 통해 LLM의 hard negatives에 대한 내성을 높이고, 관련성 있는 정보를 명확히 식별하도록 학습시키는 방법을 제안합니다. 이를 통해 LLM의 RAG 성능을 향상시킬 수 있습니다. 또한, 다양한 데이터 분포와 검색 알고리즘의 영향을 분석하여 RAG fine-tuning의 효과적인 전략을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:59<01:11, 14.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##  장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. 본 논문에서는 RAG-specific fine-tuning 방법을 통해 LLM의 hard negatives에 대한 내성을 높이고, 관련성 있는 정보를 명확히 식별하도록 학습시키는 방법을 제안합니다. 이를 통해 LLM의 RAG 성능을 향상시킬 수 있습니다. 또한, 다양한 데이터 분포와 검색 알고리즘의 영향을 분석하여 RAG fine-tuning의 효과적인 전략을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:17<01:02, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:31<00:45, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "**추가적으로, 논문에서는 다양한 데이터셋과 평가 방법을 사용하여 실험을 진행했습니다.** \n",
      "\n",
      "* **훈련 데이터셋:** Natural Question, Wizard of Wikipedia, FEVER, MMLU 등 다양한 유형의 데이터셋을 사용하여 LLM의 성능을 향상시키는 방법을 연구했습니다.\n",
      "* **테스트 데이터셋:** TriviaQA, PopQA, WebQuestions, HotpotQA, 2WikiMultiHopQA, Bamboogle, ASQA, T-REx, Zero-shot RE 등 다양한 RAG 관련 테스트 데이터셋을 사용하여 제안된 방법의 효과를 평가했습니다.\n",
      "* **평가 방법:** 기존의 평가 방법을 개선하여 \"hard negatives\" 문제를 고려한 새로운 평가 방법을 제안했습니다.\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:48<00:31, 15.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "**추가적으로, 논문에서는 다양한 데이터셋과 평가 방법을 사용하여 실험을 진행했습니다.** \n",
      "\n",
      "* **훈련 데이터셋:** Natural Question, Wizard of Wikipedia, FEVER, MMLU 등 다양한 유형의 데이터셋을 사용하여 LLM의 성능을 향상시키는 방법을 연구했습니다.\n",
      "* **테스트 데이터셋:** TriviaQA, PopQA, WebQuestions, HotpotQA, 2WikiMultiHopQA, Bamboogle, ASQA, T-REx, Zero-shot RE 등 다양한 RAG 관련 테스트 데이터셋을 사용하여 제안된 방법의 효과를 평가했습니다.\n",
      "* **평가 방법:** 기존의 평가 방법을 개선하여 \"hard negatives\" 문제를 고려한 새로운 평가 방법을 제안했습니다. \n",
      "\n",
      "**특히, 논문에서는 Slot Filling과 같은 구체적인 RAG 유형에 대한 분석과 평가 방법을 제시합니다.** \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:08<00:17, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아닙니다. 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "**추가적으로, 논문에서는 다양한 데이터셋과 평가 방법을 사용하여 실험을 진행했습니다.** \n",
      "\n",
      "* **훈련 데이터셋:** Natural Question, Wizard of Wikipedia, FEVER, MMLU 등 다양한 유형의 데이터셋을 사용하여 LLM의 성능을 향상시키는 방법을 연구했습니다.\n",
      "* **테스트 데이터셋:** TriviaQA, PopQA, WebQuestions, HotpotQA, 2WikiMultiHopQA, Bamboogle, ASQA, T-REx, Zero-shot RE 등 다양한 RAG 관련 테스트 데이터셋을 사용하여 제안된 방법의 효과를 평가했습니다.\n",
      "* **평가 방법:** 기존 평가 방법을 개선하여 \"hard negatives\" 문제를 고려한 새로운 평가 방법을 제안했습니다. \n",
      "\n",
      "**특히, 논문에서는 Slot Filling과 같은 구체적인 RAG 유형에 대한 분석과 평가 방법을 제시합니다.** \n",
      "\n",
      "**예를 들어, \"Fidelity Fiduciary Bank\" 질문에 대한 답변을 생성하는 과정을 설명하며, LLM이 \"hard negatives\"에 영향을 받는 것을 보여줍니다. 또한, 논문은 다양한 데이터셋과 평가 방법을 사용하여 실험을 진행했으며, 특히 Slot Filling과 같은 구체적인 RAG 유형에 대한 분석과 평가 방법을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:35<00:00, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. \n",
      "\n",
      "장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. 특히, 검색된 정보의 양이 증가할수록 생성된 출력의 품질이 처음에는 향상되지만, 이후 감소하는 경향을 보이는 문제점이 발견되었습니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아닙니다. 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "논문에서는 다양한 데이터셋과 평가 방법을 사용하여 실험을 진행했습니다. \n",
      "\n",
      "* **훈련 데이터셋:** Natural Question, Wizard of Wikipedia, FEVER, MMLU 등 다양한 유형의 데이터셋을 사용하여 LLM의 성능을 향상시키는 방법을 연구했습니다.\n",
      "* **테스트 데이터셋:** TriviaQA, PopQA, WebQuestions, HotpotQA, 2WikiMultiHopQA, Bamboogle, ASQA, T-REx, Zero-shot RE 등 다양한 RAG 관련 테스트 데이터셋을 사용하여 제안된 방법의 효과를 평가했습니다.\n",
      "* **평가 방법:** 기존 평가 방법을 개선하여 \"hard negatives\" 문제를 고려한 새로운 평가 방법을 제안했습니다. \n",
      "\n",
      "특히, 논문에서는 Slot Filling과 같은 구체적인 RAG 유형에 대한 분석과 평가 방법을 제시합니다. 예를 들어, \"Fidelity Fiduciary Bank\" 질문에 대한 답변을 생성하는 과정을 설명하며, LLM이 \"hard negatives\"에 영향을 받는 것을 보여줍니다. \n",
      "\n",
      "또한, 논문은 Mistral-Nemo-12B와 Gemini-1.0-Pro와 같은 다양한 LLM 모델을 사용하여 실험을 진행했으며, RAG-specific fine-tuning의 효과를 보여줍니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Refine Prompt\n",
    "\n",
    "refine_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''당신은 인공지능 전문가입니다.\n",
    "논문의 현재 시점까지의 한국어 요약이 주어집니다.\n",
    "이를 읽고, 새롭게 주어지는 내용과 비교하여 한국어 요약을 보완하거나 수정하세요.\n",
    "전체 요약은 10문장 이내로 작성하세요.\n",
    "'''),\n",
    "    ('user', '''현재 시점까지의 요약: {previous_summary}\n",
    "---\n",
    "새로운 내용: {new_text}''')])\n",
    "\n",
    "\n",
    "refine_chain = refine_prompt | gemma2 | StrOutputParser()\n",
    "\n",
    "for i in tqdm(range(1, len(document_list))):\n",
    "    intermediate_summary = refine_chain.invoke(\n",
    "        {'previous_summary':intermediate_summary,\n",
    "         'new_text':document_list[i].page_content})\n",
    "    print('')\n",
    "    print(intermediate_summary)\n",
    "    print('=======================')\n",
    "# 길이를 지정하지 않으면 오래 걸릴 수 있습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
